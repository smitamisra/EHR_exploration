{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEL8yvQyrTXA",
        "outputId": "bff710d3-a0a4-48e4-de47-b29e3c28ca2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.29.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, AutoTokenizer, BertModel, BertConfig, AutoModel, AdamW\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from nltk.tokenize import word_tokenize, regexp_tokenize\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yGDoEKURaYTJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL92gkwVIU55",
        "outputId": "ac199271-3c8b-483d-ae6a-10cd0bc2b363"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfbert = pd.read_csv('/content/drive/MyDrive/df_bert.csv').reset_index(drop=True).rename(columns={'discharge_instruction': 'Text'}).dropna()\n",
        "dfbert.head()\n",
        "dfbert.shape  #(263941, 12 without dropna() and 261639 after dropna() lost 2302 rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUXlV5EqQjm-",
        "outputId": "e9905bbd-3abf-48b7-a720-e5935f3fe8a2"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(261639, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfbert = dfbert.sample(10000)\n",
        "dfbert.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjptPs47eCpm",
        "outputId": "638df151-0f21-4a40-c45c-a277609a492d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_dev = train_test_split(dfbert, random_state=42, test_size=0.33, shuffle=True)"
      ],
      "metadata": {
        "id": "xHIY_uFxRUeg"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('train has (columns, variables);', df_train.shape, 'and test has (columns, variables):', df_dev.shape)\n",
        "print('train head;', df_train.head(5), '\\n -----------------\\n test head', df_dev.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JA_10dztfmd",
        "outputId": "0da0a261-00b6-4673-e461-f3ee0a172c30"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has (columns, variables); (6700, 12) and test has (columns, variables): (3300, 12)\n",
            "train head;         subject_id   hadm_id  \\\n",
            "155098    16889089  25347452   \n",
            "29674     15275541  27660589   \n",
            "243207    17083592  20333067   \n",
            "5478      11021643  21279207   \n",
            "15426     12717676  21112839   \n",
            "\n",
            "                                                     Text  icd_E11  icd_E78  \\\n",
            "155098                                              none         0        1   \n",
            "29674    you were sent to the hospital from clinic bec...        0        0   \n",
            "243207  dear you were admitted to the acute care surge...        0        0   \n",
            "5478    dear  it was a pleasure taking care of you at ...        1        1   \n",
            "15426   you were transfered to  for cardiac catheteriz...        1        0   \n",
            "\n",
            "        icd_E87  icd_F32  icd_I16  icd_I50  icd_N17  icd_Y92  icd_Z85  \n",
            "155098        0        0        0        0        0        1        0  \n",
            "29674         0        0        0        1        0        0        0  \n",
            "243207        0        0        0        0        0        1        0  \n",
            "5478          0        1        0        1        0        0        0  \n",
            "15426         0        0        0        1        0        0        1   \n",
            " -----------------\n",
            " test head         subject_id   hadm_id  \\\n",
            "17427     13071041  25909437   \n",
            "96323     16820374  28030057   \n",
            "183793    16677193  28543807   \n",
            "12689     12252962  26138806   \n",
            "225075    16860825  29546704   \n",
            "\n",
            "                                                     Text  icd_E11  icd_E78  \\\n",
            "17427   dear    it was a pleasure caring for you at   ...        1        0   \n",
            "96323   1 please return to the emergency department or...        1        1   \n",
            "183793  dear  you were admitted to the acute care trau...        0        0   \n",
            "12689    it was a pleasure participating in your care ...        0        1   \n",
            "225075  dear   it was a pleasure taking care of you at...        0        0   \n",
            "\n",
            "        icd_E87  icd_F32  icd_I16  icd_I50  icd_N17  icd_Y92  icd_Z85  \n",
            "17427         0        0        0        1        1        0        0  \n",
            "96323         0        1        0        0        0        0        0  \n",
            "183793        0        1        0        0        1        1        0  \n",
            "12689         0        0        1        1        0        0        1  \n",
            "225075        0        0        1        0        0        0        0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[df_train['Text']=='']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "K3MZEg7BrbXt",
        "outputId": "052c07b1-0122-43b0-be69-cb609ef6a8a5"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [subject_id, hadm_id, Text, icd_E11, icd_E78, icd_E87, icd_F32, icd_I16, icd_I50, icd_N17, icd_Y92, icd_Z85]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ef3a722f-1634-4355-b7f3-ab8b2c90ecd3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject_id</th>\n",
              "      <th>hadm_id</th>\n",
              "      <th>Text</th>\n",
              "      <th>icd_E11</th>\n",
              "      <th>icd_E78</th>\n",
              "      <th>icd_E87</th>\n",
              "      <th>icd_F32</th>\n",
              "      <th>icd_I16</th>\n",
              "      <th>icd_I50</th>\n",
              "      <th>icd_N17</th>\n",
              "      <th>icd_Y92</th>\n",
              "      <th>icd_Z85</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ef3a722f-1634-4355-b7f3-ab8b2c90ecd3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ef3a722f-1634-4355-b7f3-ab8b2c90ecd3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ef3a722f-1634-4355-b7f3-ab8b2c90ecd3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train.shape, df_train.columns)\n",
        "print(df_dev.shape)"
      ],
      "metadata": {
        "id": "CqpflaTKRUjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff9636ca-e2f2-491e-f692-6e214d1323e7"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6700, 12) Index(['subject_id', 'hadm_id', 'Text', 'icd_E11', 'icd_E78', 'icd_E87',\n",
            "       'icd_F32', 'icd_I16', 'icd_I50', 'icd_N17', 'icd_Y92', 'icd_Z85'],\n",
            "      dtype='object')\n",
            "(3300, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.rename(columns={'discharge_instruction': 'Text'}).reset_index(drop=True).dropna()\n",
        "df_dev = df_dev.rename(columns={'discharge_instruction': 'Text'}).reset_index(drop=True).dropna()"
      ],
      "metadata": {
        "id": "nWxBA6hda0uH"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "93m4Ux4TRUnm"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sections of config\n",
        "\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 200\n",
        "TRAIN_BATCH_SIZE = 64\n",
        "VALID_BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 2e-5\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "sVKrnqqCTrlm"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_cols = [col for col in df_train.columns if col not in ['subject_id', 'hadm_id', 'Text']]\n",
        "target_cols"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPpFwf95Trpg",
        "outputId": "e0a3eb84-fbfc-4ece-fbb0-431accebd608"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['icd_E11',\n",
              " 'icd_E78',\n",
              " 'icd_E87',\n",
              " 'icd_F32',\n",
              " 'icd_I16',\n",
              " 'icd_I50',\n",
              " 'icd_N17',\n",
              " 'icd_Y92',\n",
              " 'icd_Z85']"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.df = df\n",
        "        self.max_len = max_len\n",
        "        self.text = df.Text\n",
        "        self.tokenizer = tokenizer\n",
        "        self.targets = df[target_cols].values\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        text = self.text[index]\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "        \n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ],
      "metadata": {
        "id": "RaA0t0SATruN"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = BERTDataset(df_train, tokenizer, MAX_LEN)\n",
        "valid_dataset = BERTDataset(df_dev, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "JBJgIOtaTr1b"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.text.loc[3000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UZLgyUt8fAjC",
        "outputId": "367e481e-c4a0-4b1b-daf8-ae2baeba8c4e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'division of vascular and endovascular surgerycarotid endarterectomy surgery discharge instructions'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, \n",
        "                          num_workers=4, shuffle=True, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, \n",
        "                          num_workers=4, shuffle=False, pin_memory=True)"
      ],
      "metadata": {
        "id": "XpypjKUCb9z3"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "\n",
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.roberta = AutoModel.from_pretrained('roberta-base')\n",
        "#         self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.fc = torch.nn.Linear(768, len(target_cols))\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, features = self.roberta(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
        "#         output_2 = self.l2(output_1)\n",
        "        output = self.fc(features)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGpvjSIUb-Cb",
        "outputId": "b5b51e86-c90b-4698-be37-7b98278521a9"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ],
      "metadata": {
        "id": "tSx3sd0WdcGn"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(params =  model.parameters(), lr=LEARNING_RATE, weight_decay=1e-6)"
      ],
      "metadata": {
        "id": "m2INURFodjMf"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for _,data in enumerate(train_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        print('made it here')\n",
        "\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        if _%500 == 0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "geP-ZR8Hdqof"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "id": "H_zd-grMd0oP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811a17f0-f148-47c2-ec12-49aa7cd63c05"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "made it here\n",
            "Epoch: 0, Loss:  0.7133865356445312\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "Epoch: 1, Loss:  0.5055186748504639\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "Epoch: 2, Loss:  0.5109833478927612\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "Epoch: 3, Loss:  0.5026172399520874\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "Epoch: 4, Loss:  0.4914396107196808\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "Epoch: 5, Loss:  0.4212842881679535\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "Epoch: 6, Loss:  0.4470300078392029\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "Epoch: 7, Loss:  0.4198472797870636\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "Epoch: 8, Loss:  0.42838552594184875\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "Epoch: 9, Loss:  0.33130210638046265\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n",
            "made it here\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import gc\n",
        "# # model.cpu()\n",
        "# #del model\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Pwo1nSlDiCC9"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation():\n",
        "    model.eval()\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(valid_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets"
      ],
      "metadata": {
        "id": "rFKaUDSEmeVP"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs, targets = validation()\n",
        "outputs = np.array(outputs) >= 0.5\n",
        "accuracy = metrics.accuracy_score(targets, outputs)\n",
        "f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
        "f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
        "print(f\"Accuracy Score = {accuracy}\")\n",
        "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "print(f\"F1 Score (Macro) = {f1_score_macro}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se57lNY8mrby",
        "outputId": "bb2d75d2-9e9c-4b05-8da2-bb5f2792dfae"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score = 0.0693939393939394\n",
            "F1 Score (Micro) = 0.4169666614493661\n",
            "F1 Score (Macro) = 0.3692548966992083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owJJ_9sFm9t2",
        "outputId": "8114f48d-8787-4417-c34c-13a72cf9686b"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 1, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 1, 0, ..., 0, 1, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset.targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8SCzcLAnxTE",
        "outputId": "1a9cdcc7-b5c1-4432-b9c0-ae85054f73f7"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 1, 0, 0],\n",
              "       [1, 1, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 1, 1, 0],\n",
              "       ...,\n",
              "       [0, 0, 1, ..., 1, 1, 0],\n",
              "       [1, 1, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = valid_dataset.targets\n",
        "y_pred = outputs.astype(int)"
      ],
      "metadata": {
        "id": "ruNQS5y1orCz"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Hamming_Loss(y_true, y_pred):\n",
        "    temp=0\n",
        "    for i in range(y_true.shape[0]):\n",
        "        temp += np.size(y_true[i] == y_pred[i]) - np.count_nonzero(y_true[i] == y_pred[i])\n",
        "    return temp/(y_true.shape[0] * y_true.shape[1])\n",
        "    \n",
        "# print('Hamming_loss with the discharge_instruction as variable is', Hamming_Loss(y_true, y_pred))"
      ],
      "metadata": {
        "id": "JZi08rdTolWn"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Hamming_loss with the discharge_instruction as variable is 10000 observation:', Hamming_Loss(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z543roefo4HA",
        "outputId": "b8e1564b-9bee-49a5-a9a1-d2ccb06639d2"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hamming_loss with the discharge_instruction as variable is 10000 observation: 0.25084175084175087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model.bin')"
      ],
      "metadata": {
        "id": "GORDnfSWnCBP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}